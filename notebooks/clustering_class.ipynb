{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "- When should wandb.run finished?\n",
    "- When kernel restart, wandb.run not finished, even if I specified it in `__del__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deepchem.models:Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/ilker/miniconda3/envs/thesis-work/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "INFO:numba.cuda.cudadrv.driver:init\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from thesis_work.data import load_data, load_mixed_interacted_compounds\n",
    "from thesis_work.clustering.runner import ClusterRunner\n",
    "import pandas as pd\n",
    "\n",
    "## To disable all wandb logging\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./clustering_class.ipynb\"\n",
    "wandb_project_name = \"clustering-class-test\"\n",
    "\n",
    "random_state = 42\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_type = \"kinase\"\n",
    "# protein_type = \"protease\"\n",
    "# protein_type = \"gpcr\"\n",
    "\n",
    "protein_types = [protein_type]\n",
    "\n",
    "smiles_df = load_data(protein_type=protein_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours - Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_sample_size = 1000\n",
    "protein_types = [\"gpcr\", \"kinase\", \"protease\"]\n",
    "protein_types.sort()\n",
    "protein_labels = list(range(len(protein_types)))\n",
    "\n",
    "smiles_df = load_mixed_interacted_compounds(\n",
    "    protein_types=protein_types,\n",
    "    each_sample_size=each_sample_size,\n",
    "    random_state=random_state,\n",
    "    convert_category=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Drop invalid SMILES\n",
    "\n",
    "from thesis_work.utils import get_largest_fragment_from_smiles\n",
    "\n",
    "protein_types = [\"BBBP\"]\n",
    "\n",
    "# Read in data from MoleculeNet\n",
    "smiles_df = pd.read_csv(\n",
    "    \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\"\n",
    ")\n",
    "\n",
    "# Clean up columnn names so they are easier to interpret\n",
    "smiles_df = (\n",
    "    smiles_df[[\"smiles\", \"p_np\", \"name\"]]\n",
    "    .reset_index(drop=True)\n",
    "    .rename({\"smiles\": \"text\", \"p_np\": \"labels\"}, axis=1)\n",
    ")\n",
    "\n",
    "# Remove extra fragments in SMILES (typically salts, which are irrelevant to BBB permeability)\n",
    "smiles_df[\"text\"] = (\n",
    "    smiles_df[\"text\"].apply(get_largest_fragment_from_smiles).dropna().astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "\n",
    "# model_name = \"DeepChem/ChemBERTa-77M-MTR\"\n",
    "# model_name = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "model_name = \"ecfp\"\n",
    "# model_name = \"chemprop\"\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "n_components = 25\n",
    "\n",
    "# dimensionality_reduction_method = None\n",
    "# dimensionality_reduction_method_kwargs = None\n",
    "\n",
    "dimensionality_reduction_method = \"UMAP\"\n",
    "dimensionality_reduction_method_kwargs = {\n",
    "    \"n_components\": n_components,\n",
    "    \"n_neighbors\": 15,\n",
    "    \"min_dist\": 0.1,\n",
    "    \"metric\": \"euclidean\",\n",
    "}\n",
    "\n",
    "# dimensionality_reduction_method = \"PCA\"\n",
    "# dimensionality_reduction_method_kwargs = {\n",
    "#     \"n_components\": n_components,\n",
    "# }\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "# clustering_method = \"K-MEANS\"\n",
    "# clustering_method_kwargs = {\n",
    "#     \"init_method\": \"k-means++\",\n",
    "#     \"n_clusters\": 3,\n",
    "#     \"n_init\": 1,\n",
    "# }\n",
    "\n",
    "clustering_method = \"BUTINA\"\n",
    "clustering_method_kwargs = {\n",
    "    \"method\": \"ecfp\",  # NOTE: Should match with model name\n",
    "    \"distance_metric\": \"tanimoto\",\n",
    "    # \"method\": \"generic\",\n",
    "    # \"distance_metric\": \"euclidean\",\n",
    "    \"threshold\": 0.35,\n",
    "}\n",
    "\n",
    "# clustering_method = \"DBSCAN\"\n",
    "# clustering_method_kwargs = {\n",
    "#     \"min_samples\": 5,\n",
    "#     \"metric\": \"euclidean\",\n",
    "# }\n",
    "\n",
    "# clustering_method = \"HDBSCAN\"\n",
    "# clustering_method_kwargs = {\n",
    "#     \"min_cluster_size\": 5,\n",
    "#     \"metric\": \"euclidean\",\n",
    "# }\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "# wandb_run_name = None\n",
    "wandb_run_name = f\"\"\"\n",
    "    {clustering_method}_\n",
    "    {model_name if \"/\" not in model_name else model_name.split(\"/\")[1]}\n",
    "\"\"\"\n",
    "\n",
    "if dimensionality_reduction_method is not None:\n",
    "    wandb_run_name += f\"_{dimensionality_reduction_method}\"\n",
    "\n",
    "if dimensionality_reduction_method_kwargs is not None:\n",
    "    wandb_run_name += f\"_{dimensionality_reduction_method_kwargs['n_components']}\"\n",
    "\n",
    "# wandb_extra_configs = None\n",
    "wandb_extra_configs = {\"proteins\": protein_types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:thesis_work.clustering.runner:Disabling dimensionality reduction, since it is not working for BUTINA clustering with ecfp model\n"
     ]
    }
   ],
   "source": [
    "cluster_runner = ClusterRunner(\n",
    "    wandb_project_name=wandb_project_name,\n",
    "    wandb_run_name=wandb_run_name,\n",
    "    wandb_extra_configs=wandb_extra_configs,\n",
    "    smiles_df=smiles_df,\n",
    "    # smiles_df_path = None,\n",
    "    model_name=model_name,\n",
    "    random_state=random_state,\n",
    "    device=device,\n",
    "    dimensionality_reduction_method=dimensionality_reduction_method,\n",
    "    dimensionality_reduction_method_kwargs=dimensionality_reduction_method_kwargs,\n",
    "    clustering_method=clustering_method,\n",
    "    clustering_method_kwargs=clustering_method_kwargs,\n",
    "    clustering_evaluation_method=\"silhouette\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters = None\n",
    "n_clusters = list(range(2, 100, 3))\n",
    "\n",
    "# thresholds = None\n",
    "thresholds = [0.2, 0.35, 0.5, 0.8]\n",
    "\n",
    "# min_samples = None\n",
    "min_samples = [10, 20]\n",
    "\n",
    "# min_cluster_sizes = None\n",
    "min_cluster_sizes = [5, 10, 15, 20, 25]\n",
    "\n",
    "cluster_runner.run_clustering()\n",
    "# cluster_runner.run_multiple_clustering(n_clusters=n_clusters, thresholds=thresholds, min_cluster_sizes=min_cluster_sizes)\n",
    "\n",
    "del cluster_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cluster_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No runs to be synced.\n"
     ]
    }
   ],
   "source": [
    "!wandb sync --clean-old-hours 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
