{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/ilker/Documents/MyRepos/thesis-work/.venv/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53047, 2) (6631, 2) (6631, 2)\n"
     ]
    }
   ],
   "source": [
    "from thesis_work.chemberta.utils import load_data_splits\n",
    "\n",
    "train_df, valid_df, test_df = load_data_splits(protein_type=\"kinase\")\n",
    "\n",
    "print(train_df.shape, valid_df.shape, test_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[17, 8, 34, 3, 31, 12, 28, 20, 2],\n",
       " [15, 1, 41, 49, 47, 40, 36, 22, 43],\n",
       " [14, 0, 24, 16, 45, 9, 27, 38],\n",
       " [46, 5, 39, 21, 32, 37, 29, 4],\n",
       " [19, 23, 30, 25, 13, 42, 26, 10],\n",
       " [7, 11, 18, 44, 48, 33, 35, 6]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thesis_work.cv.split import create_folds, get_kfold_experiment_indices\n",
    "\n",
    "\n",
    "length = 50\n",
    "fold_list = create_folds(length=length)\n",
    "fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 1, 41, 49, 47, 40, 36, 22, 43, 14, 0, 24, 16, 45, 9, 27, 38, 46, 5, 39, 21, 32, 37, 29, 4, 19, 23, 30, 25, 13, 42, 26, 10] [17, 8, 34, 3, 31, 12, 28, 20, 2] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 14, 0, 24, 16, 45, 9, 27, 38, 46, 5, 39, 21, 32, 37, 29, 4, 19, 23, 30, 25, 13, 42, 26, 10] [15, 1, 41, 49, 47, 40, 36, 22, 43] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 15, 1, 41, 49, 47, 40, 36, 22, 43, 46, 5, 39, 21, 32, 37, 29, 4, 19, 23, 30, 25, 13, 42, 26, 10] [14, 0, 24, 16, 45, 9, 27, 38] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 15, 1, 41, 49, 47, 40, 36, 22, 43, 14, 0, 24, 16, 45, 9, 27, 38, 19, 23, 30, 25, 13, 42, 26, 10] [46, 5, 39, 21, 32, 37, 29, 4] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 15, 1, 41, 49, 47, 40, 36, 22, 43, 14, 0, 24, 16, 45, 9, 27, 38, 46, 5, 39, 21, 32, 37, 29, 4] [19, 23, 30, 25, 13, 42, 26, 10] [7, 11, 18, 44, 48, 33, 35, 6]\n"
     ]
    }
   ],
   "source": [
    "experiment_indices = get_kfold_experiment_indices(length=length)\n",
    "\n",
    "for experiment_index in experiment_indices:\n",
    "    train_df, valid_df, test_df = experiment_index\n",
    "    print(train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [15  1 41 49 47 40 36 22 43 14  0 24 16 45  9 27 38 46  5 39 21 32 37 29\n",
      "  4 19 23 30 25 13 42 26 10], test: [17  8 34  3 31 12 28 20  2]\n",
      "train: [17  8 34  3 31 12 28 20  2 14  0 24 16 45  9 27 38 46  5 39 21 32 37 29\n",
      "  4 19 23 30 25 13 42 26 10], test: [15  1 41 49 47 40 36 22 43]\n",
      "train: [17  8 34  3 31 12 28 20  2 15  1 41 49 47 40 36 22 43 46  5 39 21 32 37\n",
      " 29  4 19 23 30 25 13 42 26 10], test: [14  0 24 16 45  9 27 38]\n",
      "train: [17  8 34  3 31 12 28 20  2 15  1 41 49 47 40 36 22 43 14  0 24 16 45  9\n",
      " 27 38 19 23 30 25 13 42 26 10], test: [46  5 39 21 32 37 29  4]\n",
      "train: [17  8 34  3 31 12 28 20  2 15  1 41 49 47 40 36 22 43 14  0 24 16 45  9\n",
      " 27 38 46  5 39 21 32 37 29  4], test: [19 23 30 25 13 42 26 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# data = np.arange(50)\n",
    "data = [item for sublist in fold_list[:5] for item in sublist]\n",
    "data = np.array(data)\n",
    "\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Stackoverflow method](https://stackoverflow.com/questions/66131399/extracting-layer-output-from-classification-model-of-simpletransformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MLM were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from thesis_work.chemberta.utils import get_model\n",
    "\n",
    "\n",
    "model_type = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "# model: ClassificationModel = get_model(model_type=model_type)\n",
    "\n",
    "model_args = ClassificationArgs(\n",
    "    evaluate_each_epoch=True,\n",
    "    evaluate_during_training_verbose=True,\n",
    "    no_save=True,\n",
    "    num_train_epochs=10,\n",
    "    # overwrite_output_dir=True,\n",
    "    # auto_weights=True, # NOTE: Not working\n",
    "    # NOTE: Necessary for training outside of Colab\n",
    "    use_multiprocessing=False,\n",
    "    # dataloader_num_workers=0,\n",
    "    # process_count=1,\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    ")\n",
    "\n",
    "model_args_dict = {\n",
    "    \"output_hidden_states\": True\n",
    "}\n",
    "\n",
    "model = ClassificationModel(\n",
    "    model_type=\"roberta\",\n",
    "    model_name=model_type,\n",
    "    # args=model_args,\n",
    "    args=model_args_dict,\n",
    "    # use_cuda=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': ClassificationArgs(adafactor_beta1=None, adafactor_clip_threshold=1.0, adafactor_decay_rate=-0.8, adafactor_eps=(1e-30, 0.001), adafactor_relative_step=True, adafactor_scale_parameter=True, adafactor_warmup_init=True, adam_betas=(0.9, 0.999), adam_epsilon=1e-08, best_model_dir='outputs/best_model', cache_dir='cache_dir/', config={}, cosine_schedule_num_cycles=0.5, custom_layer_parameters=[], custom_parameter_groups=[], dataloader_num_workers=0, do_lower_case=False, dynamic_quantize=False, early_stopping_consider_epochs=False, early_stopping_delta=0, early_stopping_metric='eval_loss', early_stopping_metric_minimize=True, early_stopping_patience=3, encoding=None, eval_batch_size=8, evaluate_during_training=False, evaluate_during_training_silent=True, evaluate_during_training_steps=2000, evaluate_during_training_verbose=True, evaluate_each_epoch=True, fp16=True, gradient_accumulation_steps=1, learning_rate=4e-05, local_rank=-1, logging_steps=50, loss_type=None, loss_args={}, manual_seed=None, max_grad_norm=1.0, max_seq_length=128, model_name='DeepChem/ChemBERTa-77M-MLM', model_type='roberta', multiprocessing_chunksize=-1, n_gpu=1, no_cache=False, no_save=True, not_saved_args=[], num_train_epochs=10, optimizer='AdamW', output_dir='outputs/', overwrite_output_dir=False, polynomial_decay_schedule_lr_end=1e-07, polynomial_decay_schedule_power=1.0, process_count=14, quantized_model=False, reprocess_input_data=True, save_best_model=True, save_eval_checkpoints=True, save_model_every_epoch=True, save_optimizer_and_scheduler=True, save_steps=2000, scheduler='linear_schedule_with_warmup', silent=False, skip_special_tokens=True, tensorboard_dir=None, thread_count=None, tokenizer_name='DeepChem/ChemBERTa-77M-MLM', tokenizer_type=None, train_batch_size=8, train_custom_parameters_only=False, use_cached_eval_features=False, use_early_stopping=False, use_hf_datasets=False, use_multiprocessing=False, use_multiprocessing_for_evaluation=False, wandb_kwargs={}, wandb_project=None, warmup_ratio=0.06, warmup_steps=0, weight_decay=0.0, model_class='ClassificationModel', labels_list=[0, 1], labels_map={}, lazy_delimiter='\\t', lazy_labels_column=1, lazy_loading=False, lazy_loading_start_line=1, lazy_text_a_column=None, lazy_text_b_column=None, lazy_text_column=0, onnx=False, regression=False, sliding_window=False, special_tokens_list=[], stride=0.8, tie_value=1),\n",
       " 'is_sweeping': False,\n",
       " 'config': RobertaConfig {\n",
       "   \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MLM\",\n",
       "   \"architectures\": [\n",
       "     \"RobertaForMaskedLM\"\n",
       "   ],\n",
       "   \"attention_probs_dropout_prob\": 0.109,\n",
       "   \"bos_token_id\": 0,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.144,\n",
       "   \"hidden_size\": 384,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 464,\n",
       "   \"is_gpu\": true,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"max_position_embeddings\": 515,\n",
       "   \"model_type\": \"roberta\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 3,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"transformers_version\": \"4.27.4\",\n",
       "   \"type_vocab_size\": 1,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 600\n",
       " },\n",
       " 'num_labels': 2,\n",
       " 'weight': None,\n",
       " 'device': device(type='cuda'),\n",
       " 'loss_fct': None,\n",
       " 'model': RobertaForSequenceClassification(\n",
       "   (roberta): RobertaModel(\n",
       "     (embeddings): RobertaEmbeddings(\n",
       "       (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "       (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "       (token_type_embeddings): Embedding(1, 384)\n",
       "       (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.144, inplace=False)\n",
       "     )\n",
       "     (encoder): RobertaEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-2): 3 x RobertaLayer(\n",
       "           (attention): RobertaAttention(\n",
       "             (self): RobertaSelfAttention(\n",
       "               (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (dropout): Dropout(p=0.109, inplace=False)\n",
       "             )\n",
       "             (output): RobertaSelfOutput(\n",
       "               (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.144, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): RobertaIntermediate(\n",
       "             (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): RobertaOutput(\n",
       "             (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "             (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.144, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (classifier): RobertaClassificationHead(\n",
       "     (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "     (dropout): Dropout(p=0.144, inplace=False)\n",
       "     (out_proj): Linear(in_features=384, out_features=2, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'results': {},\n",
       " 'tokenizer': RobertaTokenizerFast(name_or_path='DeepChem/ChemBERTa-77M-MLM', vocab_size=591, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False)})}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base', return_dict=True)\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "outputs\n",
    "## last_layer_features = outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilker/miniconda3/envs/thesis-work/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.0577,  0.0297],\n",
       "         [ 0.0127, -0.0241],\n",
       "         [ 0.1406,  0.0758],\n",
       "         [ 0.0670,  0.0700],\n",
       "         [-0.0028,  0.1596],\n",
       "         [-0.2178, -0.1343],\n",
       "         [-0.0591,  0.1917],\n",
       "         [ 0.0468,  0.0645],\n",
       "         [-0.0216,  0.1594],\n",
       "         [-0.1484,  0.1124],\n",
       "         [-0.0207,  0.1587],\n",
       "         [ 0.0078,  0.1441],\n",
       "         [-0.0238, -0.0986],\n",
       "         [-0.0262, -0.1147],\n",
       "         [-0.1084, -0.0714],\n",
       "         [-0.0500,  0.0045],\n",
       "         [ 0.0169,  0.0943],\n",
       "         [ 0.2645,  0.1887],\n",
       "         [-0.0382,  0.0958],\n",
       "         [ 0.0390,  0.0884],\n",
       "         [ 0.0404,  0.0604],\n",
       "         [-0.0943, -0.0544],\n",
       "         [-0.1951,  0.1104],\n",
       "         [-0.0741, -0.1171],\n",
       "         [-0.0523, -0.0024],\n",
       "         [-0.2502, -0.0076],\n",
       "         [ 0.0089, -0.0436],\n",
       "         [-0.0857, -0.0307],\n",
       "         [-0.1610,  0.0350],\n",
       "         [-0.0249,  0.1610],\n",
       "         [-0.0710, -0.0180],\n",
       "         [-0.1207,  0.0441],\n",
       "         [-0.1839,  0.1096],\n",
       "         [-0.0554,  0.1839],\n",
       "         [ 0.0124,  0.1392],\n",
       "         [ 0.0486, -0.0087],\n",
       "         [-0.0284,  0.0924],\n",
       "         [-0.1297,  0.0055],\n",
       "         [-0.1796, -0.0920],\n",
       "         [-0.0517,  0.0569],\n",
       "         [ 0.0227,  0.0975],\n",
       "         [-0.0594,  0.1315],\n",
       "         [ 0.0499,  0.1329],\n",
       "         [ 0.1404,  0.0908],\n",
       "         [ 0.1194, -0.0088],\n",
       "         [-0.0037,  0.2032],\n",
       "         [ 0.1259,  0.1272],\n",
       "         [ 0.0974,  0.0295],\n",
       "         [-0.0582, -0.0061],\n",
       "         [-0.0042,  0.1121],\n",
       "         [ 0.1073,  0.1139],\n",
       "         [-0.1205,  0.0687],\n",
       "         [-0.0830,  0.0323],\n",
       "         [-0.1709, -0.0123]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, RobertaModel\n",
    "\n",
    "model_name = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "smiles = \"O=C(Cc1cccc2ccccc12)Nc1n[nH]c2ccc(N3CCCS3(=O)=O)cc12\" # label: 1\n",
    "\n",
    "# model = RobertaModel.from_pretrained(model_name)\n",
    "model = RobertaForTokenClassification.from_pretrained(model_name, return_dict=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(smiles, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
