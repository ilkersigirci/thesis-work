{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/home/ilker/Documents/MyRepos/thesis-work/.venv/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53047, 2) (6631, 2) (6631, 2)\n"
     ]
    }
   ],
   "source": [
    "from thesis_work.chemberta.utils import load_data_splits\n",
    "\n",
    "train_df, valid_df, test_df = load_data_splits(protein_type=\"kinase\")\n",
    "\n",
    "print(train_df.shape, valid_df.shape, test_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[17, 8, 34, 3, 31, 12, 28, 20, 2],\n",
       " [15, 1, 41, 49, 47, 40, 36, 22, 43],\n",
       " [14, 0, 24, 16, 45, 9, 27, 38],\n",
       " [46, 5, 39, 21, 32, 37, 29, 4],\n",
       " [19, 23, 30, 25, 13, 42, 26, 10],\n",
       " [7, 11, 18, 44, 48, 33, 35, 6]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thesis_work.cv.split import create_folds, get_kfold_experiment_indices\n",
    "\n",
    "\n",
    "length = 50\n",
    "fold_list = create_folds(length=length)\n",
    "fold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 1, 41, 49, 47, 40, 36, 22, 43, 14, 0, 24, 16, 45, 9, 27, 38, 46, 5, 39, 21, 32, 37, 29, 4, 19, 23, 30, 25, 13, 42, 26, 10] [17, 8, 34, 3, 31, 12, 28, 20, 2] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 14, 0, 24, 16, 45, 9, 27, 38, 46, 5, 39, 21, 32, 37, 29, 4, 19, 23, 30, 25, 13, 42, 26, 10] [15, 1, 41, 49, 47, 40, 36, 22, 43] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 15, 1, 41, 49, 47, 40, 36, 22, 43, 46, 5, 39, 21, 32, 37, 29, 4, 19, 23, 30, 25, 13, 42, 26, 10] [14, 0, 24, 16, 45, 9, 27, 38] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 15, 1, 41, 49, 47, 40, 36, 22, 43, 14, 0, 24, 16, 45, 9, 27, 38, 19, 23, 30, 25, 13, 42, 26, 10] [46, 5, 39, 21, 32, 37, 29, 4] [7, 11, 18, 44, 48, 33, 35, 6]\n",
      "[17, 8, 34, 3, 31, 12, 28, 20, 2, 15, 1, 41, 49, 47, 40, 36, 22, 43, 14, 0, 24, 16, 45, 9, 27, 38, 46, 5, 39, 21, 32, 37, 29, 4] [19, 23, 30, 25, 13, 42, 26, 10] [7, 11, 18, 44, 48, 33, 35, 6]\n"
     ]
    }
   ],
   "source": [
    "experiment_indices = get_kfold_experiment_indices(length=length)\n",
    "\n",
    "for experiment_index in experiment_indices:\n",
    "    train_df, valid_df, test_df = experiment_index\n",
    "    print(train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [15  1 41 49 47 40 36 22 43 14  0 24 16 45  9 27 38 46  5 39 21 32 37 29\n",
      "  4 19 23 30 25 13 42 26 10], test: [17  8 34  3 31 12 28 20  2]\n",
      "train: [17  8 34  3 31 12 28 20  2 14  0 24 16 45  9 27 38 46  5 39 21 32 37 29\n",
      "  4 19 23 30 25 13 42 26 10], test: [15  1 41 49 47 40 36 22 43]\n",
      "train: [17  8 34  3 31 12 28 20  2 15  1 41 49 47 40 36 22 43 46  5 39 21 32 37\n",
      " 29  4 19 23 30 25 13 42 26 10], test: [14  0 24 16 45  9 27 38]\n",
      "train: [17  8 34  3 31 12 28 20  2 15  1 41 49 47 40 36 22 43 14  0 24 16 45  9\n",
      " 27 38 19 23 30 25 13 42 26 10], test: [46  5 39 21 32 37 29  4]\n",
      "train: [17  8 34  3 31 12 28 20  2 15  1 41 49 47 40 36 22 43 14  0 24 16 45  9\n",
      " 27 38 46  5 39 21 32 37 29  4], test: [19 23 30 25 13 42 26 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# data = np.arange(50)\n",
    "data = [item for sublist in fold_list[:5] for item in sublist]\n",
    "data = np.array(data)\n",
    "\n",
    "for train, test in kfold.split(data):\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of SMILES compounds\n",
    "smiles_compounds = [\n",
    "    \"O=C(Cc1cccc2ccccc12)Nc1n[nH]c2ccc(N3CCCS3(=O)=O)cc12\",\n",
    "    \"COC(=O)NC[C@@H](NC(=O)c1ccc(-c2nc(C3CCOCC3)cnc2N)cc1F)c1cccc(Br)c1\",\n",
    "    \"COc1ccccc1Nc1cc(Oc2cc(C)c(C)nc2-c2ccccn2)ccn1\",\n",
    "    \"O=C(/C=C/CN1CCCC1)N1CCOc2cc3ncnc(Nc4ccc(F)c(Cl)c4)c3cc21\",\n",
    "    \"O=C(Nc1cccc(Nc2cc3c(=O)[nH][nH]c(=O)c3cc2Cl)c1)c1cccc(Cl)c1\",\n",
    "    \"Cc1cc(CNc2nc(Nc3cc(C4CC4)[nH]n3)cc(NC3CC4CCC(C3)N4C)n2)on1\",\n",
    "    \"Cc1cc(-c2cc(O)ccc2Cl)cc2nnc(Nc3ccc(S(N)(=O)=O)cc3)nc12\",\n",
    "    \"NS(=O)(=O)c1cccc(N/C=C2\\C(=O)Nc3ccccc32)c1\",\n",
    "    \"CC(=O)Nc1ccc2cnn(-c3cc(NC4CC4)n4ncc(C#N)c4n3)c2c1\",\n",
    "    \"CS(=O)(=O)c1cccc(Nc2nccc(N(CC#N)c3c(Cl)ccc4c3OCO4)n2)c1\",\n",
    "    \"Cc1cnc(-c2ccnc(C(C)(C)O)n2)cc1-n1c(C)cc(OCc2ccc(F)cc2F)c(Cl)c1=O\",\n",
    "    \"Cc1ccc(C(=O)Nc2cc(C(F)(F)F)ccn2)cc1/C=C/n1cnc2cncnc21\",\n",
    "    \"CNC(=O)c1cnn2ccc(N3C[C@@H](O)C[C@@H]3c3cccc(F)c3)nc12\",\n",
    "    \"COc1cc2c(cc1OC1CCOC1)Cc1c-2n[nH]c1-c1ccc(C#N)cc1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta tests for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0623,  0.0692, -0.0055,  ..., -0.0382, -0.0346, -0.0561],\n",
       "         [ 0.3130, -0.1030, -0.0109,  ...,  0.2475,  0.3796,  0.1759],\n",
       "         [ 0.1088, -0.0836,  0.0913,  ...,  0.0422,  0.1533, -0.1626],\n",
       "         [ 0.0239,  0.0840,  0.1428,  ...,  0.2742,  0.2438, -0.2356],\n",
       "         [-0.0349,  0.1832,  0.0688,  ..., -0.0401,  0.0425, -0.0353],\n",
       "         [-0.0669,  0.0724, -0.0352,  ..., -0.0702, -0.0273, -0.1019]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaModel, AutoTokenizer\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "# sentences = [\"apples taste good\", \"monkeys like bananas\", \"dogs are nice\"]\n",
    "sentences = \"apples taste good\"\n",
    "model_inputs = tokenizer(sentences, return_tensors=\"pt\")\n",
    "outputs = model(**model_inputs)\n",
    "# outputs\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base', return_dict=True)\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "outputs\n",
    "## last_layer_features = outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    " \n",
    " \n",
    "def get_word_idx(sent: str, word: str):\n",
    "    return sent.split(\" \").index(word)\n",
    "\n",
    "\n",
    "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "    Select only those subword token outputs that belong to our word of interest\n",
    "    and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n",
    "\n",
    "\n",
    "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
    "    \"\"\"\n",
    "    Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "    that make up the word of interest, and then `get_hidden_states`.\n",
    "\n",
    "    NOTE: `BertTokenizer` doesn't support `word_ids`. Use `BertTokenizerFast` instead.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "    # get all token idxs that belong to the word of interest\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "    return get_hidden_states(encoded, token_ids_word, model, layers)\n",
    "\n",
    "\n",
    "def main(layers=None):\n",
    "    # Use last four layers by default\n",
    "    layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    model = AutoModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "\n",
    "    sent = \"I like cookies .\" \n",
    "    idx = get_word_idx(sent, \"cookies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MLM were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 65, 384])\n",
      "torch.Size([65, 384])\n"
     ]
    }
   ],
   "source": [
    "# Using Deepchem\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "import torch\n",
    "\n",
    "model_name = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained('seyonec/PubChem10M_SMILES_BPE_450k')\n",
    "# model = RobertaModel.from_pretrained('seyonec/ChemBERTa-77M-MLM', output_hidden_states = True)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name, output_hidden_states = True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(smiles_compounds, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "\n",
    "states = out.hidden_states[-1].squeeze()\n",
    "\n",
    "print(states.shape)\n",
    "print(states[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Stackoverflow method](https://stackoverflow.com/questions/66131399/extracting-layer-output-from-classification-model-of-simpletransformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MLM were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from thesis_work.chemberta.utils import get_model\n",
    "\n",
    "\n",
    "model_type = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "# model: ClassificationModel = get_model(model_type=model_type)\n",
    "\n",
    "model_args = ClassificationArgs(\n",
    "    evaluate_each_epoch=True,\n",
    "    evaluate_during_training_verbose=True,\n",
    "    no_save=True,\n",
    "    num_train_epochs=10,\n",
    "    # overwrite_output_dir=True,\n",
    "    # auto_weights=True, # NOTE: Not working\n",
    "    # NOTE: Necessary for training outside of Colab\n",
    "    use_multiprocessing=False,\n",
    "    # dataloader_num_workers=0,\n",
    "    # process_count=1,\n",
    "    use_multiprocessing_for_evaluation=False,\n",
    ")\n",
    "\n",
    "model_args_dict = {\n",
    "    \"output_hidden_states\": True\n",
    "}\n",
    "\n",
    "model = ClassificationModel(\n",
    "    model_type=\"roberta\",\n",
    "    model_name=model_type,\n",
    "    # args=model_args,\n",
    "    args=model_args_dict,\n",
    "    # use_cuda=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': ClassificationArgs(adafactor_beta1=None, adafactor_clip_threshold=1.0, adafactor_decay_rate=-0.8, adafactor_eps=(1e-30, 0.001), adafactor_relative_step=True, adafactor_scale_parameter=True, adafactor_warmup_init=True, adam_betas=(0.9, 0.999), adam_epsilon=1e-08, best_model_dir='outputs/best_model', cache_dir='cache_dir/', config={}, cosine_schedule_num_cycles=0.5, custom_layer_parameters=[], custom_parameter_groups=[], dataloader_num_workers=0, do_lower_case=False, dynamic_quantize=False, early_stopping_consider_epochs=False, early_stopping_delta=0, early_stopping_metric='eval_loss', early_stopping_metric_minimize=True, early_stopping_patience=3, encoding=None, eval_batch_size=8, evaluate_during_training=False, evaluate_during_training_silent=True, evaluate_during_training_steps=2000, evaluate_during_training_verbose=True, evaluate_each_epoch=True, fp16=True, gradient_accumulation_steps=1, learning_rate=4e-05, local_rank=-1, logging_steps=50, loss_type=None, loss_args={}, manual_seed=None, max_grad_norm=1.0, max_seq_length=128, model_name='DeepChem/ChemBERTa-77M-MLM', model_type='roberta', multiprocessing_chunksize=-1, n_gpu=1, no_cache=False, no_save=True, not_saved_args=[], num_train_epochs=10, optimizer='AdamW', output_dir='outputs/', overwrite_output_dir=False, polynomial_decay_schedule_lr_end=1e-07, polynomial_decay_schedule_power=1.0, process_count=14, quantized_model=False, reprocess_input_data=True, save_best_model=True, save_eval_checkpoints=True, save_model_every_epoch=True, save_optimizer_and_scheduler=True, save_steps=2000, scheduler='linear_schedule_with_warmup', silent=False, skip_special_tokens=True, tensorboard_dir=None, thread_count=None, tokenizer_name='DeepChem/ChemBERTa-77M-MLM', tokenizer_type=None, train_batch_size=8, train_custom_parameters_only=False, use_cached_eval_features=False, use_early_stopping=False, use_hf_datasets=False, use_multiprocessing=False, use_multiprocessing_for_evaluation=False, wandb_kwargs={}, wandb_project=None, warmup_ratio=0.06, warmup_steps=0, weight_decay=0.0, model_class='ClassificationModel', labels_list=[0, 1], labels_map={}, lazy_delimiter='\\t', lazy_labels_column=1, lazy_loading=False, lazy_loading_start_line=1, lazy_text_a_column=None, lazy_text_b_column=None, lazy_text_column=0, onnx=False, regression=False, sliding_window=False, special_tokens_list=[], stride=0.8, tie_value=1),\n",
       " 'is_sweeping': False,\n",
       " 'config': RobertaConfig {\n",
       "   \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MLM\",\n",
       "   \"architectures\": [\n",
       "     \"RobertaForMaskedLM\"\n",
       "   ],\n",
       "   \"attention_probs_dropout_prob\": 0.109,\n",
       "   \"bos_token_id\": 0,\n",
       "   \"classifier_dropout\": null,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_act\": \"gelu\",\n",
       "   \"hidden_dropout_prob\": 0.144,\n",
       "   \"hidden_size\": 384,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": 464,\n",
       "   \"is_gpu\": true,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"max_position_embeddings\": 515,\n",
       "   \"model_type\": \"roberta\",\n",
       "   \"num_attention_heads\": 12,\n",
       "   \"num_hidden_layers\": 3,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"position_embedding_type\": \"absolute\",\n",
       "   \"transformers_version\": \"4.27.4\",\n",
       "   \"type_vocab_size\": 1,\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 600\n",
       " },\n",
       " 'num_labels': 2,\n",
       " 'weight': None,\n",
       " 'device': device(type='cuda'),\n",
       " 'loss_fct': None,\n",
       " 'model': RobertaForSequenceClassification(\n",
       "   (roberta): RobertaModel(\n",
       "     (embeddings): RobertaEmbeddings(\n",
       "       (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "       (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "       (token_type_embeddings): Embedding(1, 384)\n",
       "       (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.144, inplace=False)\n",
       "     )\n",
       "     (encoder): RobertaEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-2): 3 x RobertaLayer(\n",
       "           (attention): RobertaAttention(\n",
       "             (self): RobertaSelfAttention(\n",
       "               (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (dropout): Dropout(p=0.109, inplace=False)\n",
       "             )\n",
       "             (output): RobertaSelfOutput(\n",
       "               (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.144, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): RobertaIntermediate(\n",
       "             (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): RobertaOutput(\n",
       "             (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "             (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.144, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (classifier): RobertaClassificationHead(\n",
       "     (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "     (dropout): Dropout(p=0.144, inplace=False)\n",
       "     (out_proj): Linear(in_features=384, out_features=2, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'results': {},\n",
       " 'tokenizer': RobertaTokenizerFast(name_or_path='DeepChem/ChemBERTa-77M-MLM', vocab_size=591, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=False)})}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilker/miniconda3/envs/thesis-work/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.0577,  0.0297],\n",
       "         [ 0.0127, -0.0241],\n",
       "         [ 0.1406,  0.0758],\n",
       "         [ 0.0670,  0.0700],\n",
       "         [-0.0028,  0.1596],\n",
       "         [-0.2178, -0.1343],\n",
       "         [-0.0591,  0.1917],\n",
       "         [ 0.0468,  0.0645],\n",
       "         [-0.0216,  0.1594],\n",
       "         [-0.1484,  0.1124],\n",
       "         [-0.0207,  0.1587],\n",
       "         [ 0.0078,  0.1441],\n",
       "         [-0.0238, -0.0986],\n",
       "         [-0.0262, -0.1147],\n",
       "         [-0.1084, -0.0714],\n",
       "         [-0.0500,  0.0045],\n",
       "         [ 0.0169,  0.0943],\n",
       "         [ 0.2645,  0.1887],\n",
       "         [-0.0382,  0.0958],\n",
       "         [ 0.0390,  0.0884],\n",
       "         [ 0.0404,  0.0604],\n",
       "         [-0.0943, -0.0544],\n",
       "         [-0.1951,  0.1104],\n",
       "         [-0.0741, -0.1171],\n",
       "         [-0.0523, -0.0024],\n",
       "         [-0.2502, -0.0076],\n",
       "         [ 0.0089, -0.0436],\n",
       "         [-0.0857, -0.0307],\n",
       "         [-0.1610,  0.0350],\n",
       "         [-0.0249,  0.1610],\n",
       "         [-0.0710, -0.0180],\n",
       "         [-0.1207,  0.0441],\n",
       "         [-0.1839,  0.1096],\n",
       "         [-0.0554,  0.1839],\n",
       "         [ 0.0124,  0.1392],\n",
       "         [ 0.0486, -0.0087],\n",
       "         [-0.0284,  0.0924],\n",
       "         [-0.1297,  0.0055],\n",
       "         [-0.1796, -0.0920],\n",
       "         [-0.0517,  0.0569],\n",
       "         [ 0.0227,  0.0975],\n",
       "         [-0.0594,  0.1315],\n",
       "         [ 0.0499,  0.1329],\n",
       "         [ 0.1404,  0.0908],\n",
       "         [ 0.1194, -0.0088],\n",
       "         [-0.0037,  0.2032],\n",
       "         [ 0.1259,  0.1272],\n",
       "         [ 0.0974,  0.0295],\n",
       "         [-0.0582, -0.0061],\n",
       "         [-0.0042,  0.1121],\n",
       "         [ 0.1073,  0.1139],\n",
       "         [-0.1205,  0.0687],\n",
       "         [-0.0830,  0.0323],\n",
       "         [-0.1709, -0.0123]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, RobertaModel\n",
    "\n",
    "model_name = \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "smiles = \"O=C(Cc1cccc2ccccc12)Nc1n[nH]c2ccc(N3CCCS3(=O)=O)cc12\" # label: 1\n",
    "\n",
    "# model = RobertaModel.from_pretrained(model_name)\n",
    "model = RobertaForTokenClassification.from_pretrained(model_name, return_dict=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(smiles, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
